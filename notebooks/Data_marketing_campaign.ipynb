{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6eef7ff3-592b-4cdc-b66a-2fe040601619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All cleaned datasets have been loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from sqlalchemy import create_engine\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from urllib.parse import quote_plus\n",
    "import requests\n",
    "from datetime import datetime\n",
    "\n",
    "import logging\n",
    "from google.cloud import bigquery\n",
    "\n",
    "\n",
    "# Define the base path \n",
    "clean_data_path = os.path.join(\"..\", \"data\", \"clean\")\n",
    "\n",
    "# 1. Read UCI Bank Marketing data\n",
    "uci_df = pd.read_csv(os.path.join(clean_data_path, \"uci_bank_marketing_cleaned.csv\"))\n",
    "\n",
    "# 2. Read Campaign Dimension data\n",
    "campaign_df = pd.read_csv(os.path.join(clean_data_path, \"campaign_dim_cleaned.csv\"))\n",
    "\n",
    "# 3. Read ECB Interest Rates data\n",
    "ecb_df = pd.read_csv(os.path.join(clean_data_path, \"ecb_interest_rates_cleaned.csv\"))\n",
    "\n",
    "# 4. Read Final Master DataFrame\n",
    "final_df = pd.read_csv(os.path.join(clean_data_path, \" Marketing_Campaign_final.csv\"))\n",
    "\n",
    "print(\"All cleaned datasets have been loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d806c0-88a3-4d13-b154-97c8a10e9935",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f408d90e-fa28-42c0-971c-0394ac268330",
   "metadata": {},
   "source": [
    "### API Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f536cf2-6fa5-4874-b716-dd0f2a61df2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define the API address (The 'base' URL from your terminal)\n",
    "BASE_URL = \"http://127.0.0.1:8000\"\n",
    "\n",
    "# 2. Call the KPI Analytics endpoint\n",
    "print(\"--- Testing KPI Endpoint ---\")\n",
    "try:\n",
    "    response_kpi = requests.get(f\"{BASE_URL}/kpi/conversion\")\n",
    "    if response_kpi.status_code == 200:\n",
    "        print(\"Success! Data received:\")\n",
    "        print(response_kpi.json())\n",
    "    else:\n",
    "        print(f\"Error {response_kpi.status_code}: {response_kpi.text}\")\n",
    "except Exception as e:\n",
    "    print(f\"Connection failed: {e}\")\n",
    "\n",
    "# 3. Call the Clients endpoint and turn it into a DataFrame\n",
    "print(\"\\n--- Testing Clients Endpoint (with filters) ---\")\n",
    "# We will use the 'limit' and 'job' parameters defined in the FastAPI code\n",
    "params = {\"limit\": 10, \"job\": \"management\"} \n",
    "\n",
    "response_clients = requests.get(f\"{BASE_URL}/clients\", params=params)\n",
    "\n",
    "if response_clients.status_code == 200:\n",
    "    data = response_clients.json()\n",
    "    df_api_results = pd.DataFrame(data)\n",
    "    print(\"Top 5 clients from API:\")\n",
    "    display(df_api_results.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3115a4c9-6037-4a3c-8093-a36d86bda493",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c8b11ed6-df42-4cda-b99e-eb4489e739df",
   "metadata": {},
   "source": [
    "### Part 2: Big Data System (BigQuery)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361f5a52-572d-4268-b5e4-9782ddebf6b9",
   "metadata": {},
   "source": [
    "#### Step 1: Create the \"Master Table\" (Denormalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996a848f-3ea7-410c-83d1-9eb64a1b1cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. PREPARE DENORMALIZED DATA\n",
    "\n",
    "\n",
    "def get_denormalized_data(uci_df, campaign_df, ecb_df):\n",
    "    \"\"\"\n",
    "    Combines normalized DataFrames into a single Wide Table for BigQuery.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Starting Denormalization for BigQuery ---\")\n",
    "\n",
    "    # A. Prepare Dates & IDs (Re-using your existing logic)\n",
    "    ecb_df['date'] = pd.to_datetime(ecb_df['date'])\n",
    "    ecb_df['economics_id'] = ecb_df['date'].dt.strftime('%Y%m%d').astype(int)\n",
    "    \n",
    "    uci_df['call_date'] = pd.to_datetime(dict(year=uci_df.year, month=uci_df.month_num, day=uci_df.last_contact_day))\n",
    "    uci_df['economics_id'] = uci_df['call_date'].dt.strftime('%Y%m%d').astype(int)\n",
    "    \n",
    "    campaign_df['campaign_id'] = range(1, len(campaign_df) + 1)\n",
    "    campaign_df['campaign_start_date'] = pd.to_datetime(campaign_df['campaign_start_date'])\n",
    "    campaign_df['campaign_end_date'] = pd.to_datetime(campaign_df['campaign_end_date'])\n",
    "\n",
    "    # B. Map Campaign IDs to UCI Interactions\n",
    "    uci_df['campaign_id'] = 1 \n",
    "    for _, row in campaign_df.iterrows():\n",
    "        mask = (uci_df['call_date'] >= row['campaign_start_date']) & (uci_df['call_date'] <= row['campaign_end_date'])\n",
    "        uci_df.loc[mask, 'campaign_id'] = row['campaign_id']\n",
    "\n",
    "    # C. Execute Merges (The Denormalization)\n",
    "    # Join Interactions with Campaigns\n",
    "    wide_df = uci_df.merge(campaign_df, on='campaign_id', how='left', suffixes=('', '_drop'))\n",
    "    \n",
    "    # Join with Economics (Interest Rates)\n",
    "    wide_df = wide_df.merge(ecb_df, on='economics_id', how='left', suffixes=('', '_drop'))\n",
    "\n",
    "    # D. Clean up duplicated columns from joins\n",
    "    wide_df = wide_df.loc[:, ~wide_df.columns.str.contains('_drop')]\n",
    "    \n",
    "    print(f\"Denormalized Table Created: {wide_df.shape[1]} columns and {len(wide_df)} rows.\")\n",
    "    return wide_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70c915d6-60ff-4b7a-8599-d6b60c157adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Denormalization for BigQuery ---\n",
      "Denormalized Table Created: 30 columns and 45211 rows.\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 3. EXECUTION\n",
    "# ==========================================\n",
    "from BigQuery_denormalized_data import get_denormalized_data, load_to_bigquery\n",
    "# Assuming uci_df, campaign_df, ecb_df are already loaded from your ETL script\n",
    "wide_table = get_denormalized_data(uci_df, campaign_df, ecb_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4096e4b4-2405-4621-bf3f-1605e133999d",
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_table.to_csv(\"../data/clean/denormalized_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d450cb79-cdd7-419b-9a23-116b7c8b6014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the function with your correct dataset\n",
    "from BigQuery_denormalized_data import perform_clean_upload\n",
    "perform_clean_upload(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc8426e-0017-4964-b8f7-7dce556aea0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. PREPARE DATA FOR PARTITIONING ---\n",
    "# BigQuery requires a proper datetime object for time-based partitioning\n",
    "import pandas as pd\n",
    "wide_df['call_date'] = pd.to_datetime(wide_df['call_date'])\n",
    "\n",
    "# --- 2. CONFIGURATION ---\n",
    "MY_PROJECT_ID = \"rncp-bank-marketing\"  \n",
    "MY_DATASET_ID = \"bank_analytics\"       \n",
    "MY_TABLE_NAME = \"denormalized_marketing\"\n",
    "MY_KEY_FILE   = \"key.json.json\"       \n",
    "\n",
    "# --- 3. EXECUTION ---\n",
    "# This call now triggers the updated logic with Partitioning and Clustering\n",
    "result = load_to_bigquery(\n",
    "    df=wide_df, \n",
    "    project_id=MY_PROJECT_ID, \n",
    "    dataset_id=MY_DATASET_ID, \n",
    "    table_name=MY_TABLE_NAME, \n",
    "    key_filename=MY_KEY_FILE\n",
    ")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a92340-ec86-4c40-a92c-d1a730efaad1",
   "metadata": {},
   "source": [
    "## Hypothesis 5: A Machine Learning model trained on {Client + Macro + Campaign} features outperforms a model trained on {Client} features aloneÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91583af2-b077-4121-a7e0-b1909d19ba09",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_feature_set = pd.read_csv(\"../data/clean/denormalized_data.csv\")\n",
    "ml_feature_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e73d580-4845-462d-99de-c69f155226d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from machine_learning import prepare_duel_datasets\n",
    "\n",
    "# 1. PREPROCESSING: Clean and Split\n",
    "X_a, X_b, y = prepare_duel_datasets(ml_feature_set)\n",
    "\n",
    "# 3. Verify the results\n",
    "print(\"Success! Data is now preprocessed and ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e40635-bf81-4436-a76f-ff8eed198720",
   "metadata": {},
   "outputs": [],
   "source": [
    "from machine_learning import train_and_evaluate\n",
    "\n",
    "f1_client = train_and_evaluate(X_a, y, \"Model A (Client Only)\")\n",
    "f1_full = train_and_evaluate(X_b, y, \"Model B (Integrated - The Grand Finale)\")\n",
    "\n",
    "print(f\"\\n ROI Justification: Integrated features provided a {((f1_full/f1_client)-1)*100:.2f}% lift in F1-Score!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211a9d6d-74d7-4167-86af-b18e56ed6036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create the missing 'rate_bin' column\n",
    "# We divide the interest rates into 3 categories: Low, Medium, and High\n",
    "ml_feature_set['rate_bin'] = pd.qcut(ml_feature_set['ecb_rate'], q=3, labels=['Low', 'Medium', 'High'])\n",
    "\n",
    "# 2. (Optional) If you also get a KeyError for 'balance_segment', create it here:\n",
    "ml_feature_set['balance_segment'] = pd.qcut(ml_feature_set['account_balance'], q=3, labels=['Bronze', 'Silver', 'Gold'])\n",
    "\n",
    "# 3. Now call your statistical tests\n",
    "from machine_learning import test_stat_significance\n",
    "\n",
    "test_stat_significance(ml_feature_set, 'rate_bin')      # Validates H1\n",
    "test_stat_significance(ml_feature_set, 'contact_type')  # Validates H3\n",
    "test_stat_significance(ml_feature_set, 'balance_segment') # Validates H4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95dfd3a-ef3e-4ed8-beb9-c9c6dc6305eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check expected frequencies\n",
    "chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "if (expected < 5).any():\n",
    "    print(\"Warning: Some cells have too few samples. Group categories together.\")\n",
    "else:\n",
    "    print(\"Test is valid: All expected frequencies are > 5.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
