{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6eef7ff3-592b-4cdc-b66a-2fe040601619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All cleaned datasets have been loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from sqlalchemy import create_engine\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from urllib.parse import quote_plus\n",
    "import requests\n",
    "from datetime import datetime\n",
    "\n",
    "import logging\n",
    "from google.cloud import bigquery\n",
    "\n",
    "\n",
    "# Define the base path \n",
    "clean_data_path = os.path.join(\"..\", \"data\", \"clean\")\n",
    "\n",
    "# 1. Read UCI Bank Marketing data\n",
    "uci_df = pd.read_csv(os.path.join(clean_data_path, \"uci_bank_marketing_cleaned.csv\"))\n",
    "\n",
    "# 2. Read Campaign Dimension data\n",
    "campaign_df = pd.read_csv(os.path.join(clean_data_path, \"campaign_dim_cleaned.csv\"))\n",
    "\n",
    "# 3. Read ECB Interest Rates data\n",
    "ecb_df = pd.read_csv(os.path.join(clean_data_path, \"ecb_interest_rates_cleaned.csv\"))\n",
    "\n",
    "# 4. Read Final Master DataFrame\n",
    "final_df = pd.read_csv(os.path.join(clean_data_path, \" Marketing_Campaign_final.csv\"))\n",
    "\n",
    "print(\"All cleaned datasets have been loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d806c0-88a3-4d13-b154-97c8a10e9935",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "436197f1-3756-47d2-afb0-0c2ee568029b",
   "metadata": {},
   "source": [
    "# Implementation with SQLAlchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3296675-9148-4634-83dd-167be958dd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP A: Imports\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine \n",
    "\n",
    "# STEP B: The Connection String\n",
    "# This is the \"address\" of your database.\n",
    "# Format: dialect+driver://username:password@host:port/database\n",
    "db_connection_str = 'mysql+mysqlconnector://root:password@localhost/bank_marketing_dw'\n",
    "\n",
    "# STEP C: The Engine\n",
    "db_connection = create_engine(db_connection_str)\n",
    "\n",
    "# STEP D: Loading Data\n",
    "# taking the 'dim_client_df' (Pandas) and sending it to 'dim_client' (MySQL)\n",
    "dim_client_df.to_sql(\n",
    "    name='dim_client',       # 1. Target Table Name in MySQL\n",
    "    con=db_connection,       # 2. The Connection Engine\n",
    "    if_exists='append',      # 3. Behavior if table exists\n",
    "    index=False              # 4. Do not write the DataFrame index (0,1,2...) as a column\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e800bd21-f7e5-470a-be50-eba09f8e7990",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Establish Connection (Replace with your credentials)\n",
    "# Format: mysql+mysqlconnector://user:password@host:port/database_name\n",
    "db_connection_str = 'mysql+mysqlconnector://root:password@localhost/bank_marketing_dw'\n",
    "db_connection = create_engine(db_connection_str)\n",
    "\n",
    "# 2. Load Dimensions FIRST (They must exist before the Fact table links to them)\n",
    "# Note: Ensure column names in DF match MySQL table exactly!\n",
    "\n",
    "# Load Client Dimension\n",
    "dim_client_df.to_sql('dim_client', db_connection, if_exists='append', index=False)\n",
    "print(\"✅ DIM_CLIENT loaded.\")\n",
    "\n",
    "# Load Campaign Dimension\n",
    "dim_campaign_df.to_sql('dim_campaign', db_connection, if_exists='append', index=False)\n",
    "print(\"✅ DIM_CAMPAIGN loaded.\")\n",
    "\n",
    "# Load Economics Dimension\n",
    "dim_economics_df.to_sql('dim_economics', db_connection, if_exists='append', index=False)\n",
    "print(\"✅ DIM_ECONOMICS loaded.\")\n",
    "\n",
    "# 3. Load Fact Table LAST\n",
    "# Ensure your DF has the foreign keys (client_id, campaign_id, economics_id) created during cleaning\n",
    "fact_interactions_df.to_sql('fact_interactions', db_connection, if_exists='append', index=False)\n",
    "print(\"✅ FACT_INTERACTIONS loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d896eb56-ef55-4d65-912d-00c7de327882",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056e482f-7b4e-4a3f-8512-2f9cd8855f26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f408d90e-fa28-42c0-971c-0394ac268330",
   "metadata": {},
   "source": [
    "### API Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f536cf2-6fa5-4874-b716-dd0f2a61df2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define the API address (The 'base' URL from your terminal)\n",
    "BASE_URL = \"http://127.0.0.1:8000\"\n",
    "\n",
    "# 2. Call the KPI Analytics endpoint\n",
    "print(\"--- Testing KPI Endpoint ---\")\n",
    "try:\n",
    "    response_kpi = requests.get(f\"{BASE_URL}/kpi/conversion\")\n",
    "    if response_kpi.status_code == 200:\n",
    "        print(\"Success! Data received:\")\n",
    "        print(response_kpi.json())\n",
    "    else:\n",
    "        print(f\"Error {response_kpi.status_code}: {response_kpi.text}\")\n",
    "except Exception as e:\n",
    "    print(f\"Connection failed: {e}\")\n",
    "\n",
    "# 3. Call the Clients endpoint and turn it into a DataFrame\n",
    "print(\"\\n--- Testing Clients Endpoint (with filters) ---\")\n",
    "# We will use the 'limit' and 'job' parameters defined in the FastAPI code\n",
    "params = {\"limit\": 10, \"job\": \"management\"} \n",
    "\n",
    "response_clients = requests.get(f\"{BASE_URL}/clients\", params=params)\n",
    "\n",
    "if response_clients.status_code == 200:\n",
    "    data = response_clients.json()\n",
    "    df_api_results = pd.DataFrame(data)\n",
    "    print(\"Top 5 clients from API:\")\n",
    "    display(df_api_results.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3115a4c9-6037-4a3c-8093-a36d86bda493",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c8b11ed6-df42-4cda-b99e-eb4489e739df",
   "metadata": {},
   "source": [
    "### Part 2: Big Data System (BigQuery)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361f5a52-572d-4268-b5e4-9782ddebf6b9",
   "metadata": {},
   "source": [
    "#### Step 1: Create the \"Master Table\" (Denormalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996a848f-3ea7-410c-83d1-9eb64a1b1cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. PREPARE DENORMALIZED DATA\n",
    "\n",
    "\n",
    "def get_denormalized_data(uci_df, campaign_df, ecb_df):\n",
    "    \"\"\"\n",
    "    Combines normalized DataFrames into a single Wide Table for BigQuery.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Starting Denormalization for BigQuery ---\")\n",
    "\n",
    "    # A. Prepare Dates & IDs (Re-using your existing logic)\n",
    "    ecb_df['date'] = pd.to_datetime(ecb_df['date'])\n",
    "    ecb_df['economics_id'] = ecb_df['date'].dt.strftime('%Y%m%d').astype(int)\n",
    "    \n",
    "    uci_df['call_date'] = pd.to_datetime(dict(year=uci_df.year, month=uci_df.month_num, day=uci_df.last_contact_day))\n",
    "    uci_df['economics_id'] = uci_df['call_date'].dt.strftime('%Y%m%d').astype(int)\n",
    "    \n",
    "    campaign_df['campaign_id'] = range(1, len(campaign_df) + 1)\n",
    "    campaign_df['campaign_start_date'] = pd.to_datetime(campaign_df['campaign_start_date'])\n",
    "    campaign_df['campaign_end_date'] = pd.to_datetime(campaign_df['campaign_end_date'])\n",
    "\n",
    "    # B. Map Campaign IDs to UCI Interactions\n",
    "    uci_df['campaign_id'] = 1 \n",
    "    for _, row in campaign_df.iterrows():\n",
    "        mask = (uci_df['call_date'] >= row['campaign_start_date']) & (uci_df['call_date'] <= row['campaign_end_date'])\n",
    "        uci_df.loc[mask, 'campaign_id'] = row['campaign_id']\n",
    "\n",
    "    # C. Execute Merges (The Denormalization)\n",
    "    # Join Interactions with Campaigns\n",
    "    wide_df = uci_df.merge(campaign_df, on='campaign_id', how='left', suffixes=('', '_drop'))\n",
    "    \n",
    "    # Join with Economics (Interest Rates)\n",
    "    wide_df = wide_df.merge(ecb_df, on='economics_id', how='left', suffixes=('', '_drop'))\n",
    "\n",
    "    # D. Clean up duplicated columns from joins\n",
    "    wide_df = wide_df.loc[:, ~wide_df.columns.str.contains('_drop')]\n",
    "    \n",
    "    print(f\"Denormalized Table Created: {wide_df.shape[1]} columns and {len(wide_df)} rows.\")\n",
    "    return wide_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c915d6-60ff-4b7a-8599-d6b60c157adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 3. EXECUTION\n",
    "# ==========================================\n",
    "from BigQuery_denormalized_data import get_denormalized_data, load_to_bigquery\n",
    "# Assuming uci_df, campaign_df, ecb_df are already loaded from your ETL script\n",
    "wide_table = get_denormalized_data(uci_df, campaign_df, ecb_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4096e4b4-2405-4621-bf3f-1605e133999d",
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_table.to_csv(\"../data/clean/denormalized_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d450cb79-cdd7-419b-9a23-116b7c8b6014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the function with your correct dataset\n",
    "from BigQuery_denormalized_data import perform_clean_upload\n",
    "perform_clean_upload(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc8426e-0017-4964-b8f7-7dce556aea0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. PREPARE DATA FOR PARTITIONING ---\n",
    "# BigQuery requires a proper datetime object for time-based partitioning\n",
    "import pandas as pd\n",
    "wide_df['call_date'] = pd.to_datetime(wide_df['call_date'])\n",
    "\n",
    "# --- 2. CONFIGURATION ---\n",
    "MY_PROJECT_ID = \"rncp-bank-marketing\"  \n",
    "MY_DATASET_ID = \"bank_analytics\"       \n",
    "MY_TABLE_NAME = \"denormalized_marketing\"\n",
    "MY_KEY_FILE   = \"key.json.json\"       \n",
    "\n",
    "# --- 3. EXECUTION ---\n",
    "# This call now triggers the updated logic with Partitioning and Clustering\n",
    "result = load_to_bigquery(\n",
    "    df=wide_df, \n",
    "    project_id=MY_PROJECT_ID, \n",
    "    dataset_id=MY_DATASET_ID, \n",
    "    table_name=MY_TABLE_NAME, \n",
    "    key_filename=MY_KEY_FILE\n",
    ")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e799b19e-3707-427d-aa40-cd282368151b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
